# 湖仓一体指标回放：从运营复盘到数据织布

## 夜间复盘的真实问题
周一晚上 9 点，运营团队需要复盘“周末红包”活动，要求 30 分钟内给出“每 10 分钟 GMV + 转化率 + 用户画像”回放。数据平台却只能拉出仓库里的 T+1 报表，临时 SQL 甚至查不到埋点。为了补数据，分析师只能在本地拼 CSV，最终复盘拖到凌晨。这种痛点的根源是：湖仓一体没有真正落地，湖里的实时埋点与仓库的指标脱节，运营只能在 PPT 上猜想。

### 夹层里发生了什么
我们把链路拆开：埋点进入对象存储，Streaming 任务写 Delta 表，仓库再跑 ETL。每一层都可能引入延迟或字段缺失，指标口径自然对不上。解决方案不是“再建一个表”，而是把湖和仓的故事线连起来，让任何人都能回放任意时间段的指标。

image_group
![湖仓总览草图](https://images.unsplash.com/photo-1454165205744-3b78555e5572?auto=format&fit=crop&w=800&q=80)

## 心智模型：指标 = 数据织布

### 经线与纬线
把仓库想成经线（结构化指标），湖想成纬线（原始事件）。织布时必须先决定“纬线能否快速补洞”，再决定“经线如何抽象”。我们给每一张指标表附上“回放脚本”，写明它来自哪些原始事件、如何复算，这样运营在复盘时不再迷路。

### 指标会说话
指标不能只是表格，要配合时间轴、分层视图、异常标记。我们在仪表里增加“回放”按钮，一键把 12:00～13:00 的事件重放到临时表，帮助运营“看见”那一小时发生了什么。当团队开始用这张梗图互相调侃时，说明大家终于明白口径对齐的重要性。

image_group
![meme: 两个按钮难以抉择](https://i.imgflip.com/30b1gx.jpg)

image_group
![指标编织模型](https://raw.githubusercontent.com/github/explore/main/topics/data-science/data-science.png)

## 最小可复现实验：Lakehouse 回放台

### 环境与依赖
- Python 3.10+
- `pip install duckdb pandas`
- 示例数据使用 CSV/JSON，可在本地直接创建

```bash
python3 -m venv lakehouse && cd lakehouse
source bin/activate
pip install duckdb pandas
```

### 核心代码：lakehouse_lab.py
下列脚本把实时事件（JSON Lines）和交易事实表（CSV）接入 DuckDB，构建一个 10 分钟粒度的“回放指标”，并输出 GMV、转化率、热门渠道。

```python
# file: lakehouse_lab.py
import duckdb
import pandas as pd
from pathlib import Path

ORDERS_PATH = Path("orders.csv")
EVENTS_PATH = Path("events.jsonl")


def load_sources():
  orders = pd.read_csv(ORDERS_PATH, parse_dates=["paid_at"])
  events = pd.read_json(EVENTS_PATH, lines=True)
  events["ts"] = pd.to_datetime(events["ts"])
  return orders, events


def build_metrics(con):
  query = """
  WITH orders_window AS (
    SELECT DATE_TRUNC('minute', paid_at) AS minute_window,
           SUM(amount) AS gmv,
           COUNT(DISTINCT user_id) AS buyers
    FROM orders
    GROUP BY 1
  ),
  funnel AS (
    SELECT DATE_TRUNC('minute', ts) AS minute_window,
           SUM(CASE WHEN event = 'view' THEN 1 ELSE 0 END) AS views,
           SUM(CASE WHEN event = 'click' THEN 1 ELSE 0 END) AS clicks
    FROM events
    GROUP BY 1
  )
  SELECT o.minute_window,
         gmv,
         buyers,
         COALESCE(f.views, 0) AS views,
         COALESCE(f.clicks, 0) AS clicks,
         ROUND(gmv / NULLIF(buyers, 0), 2) AS arpu,
         ROUND(buyers::DOUBLE / NULLIF(f.views, 0), 4) AS conversion
  FROM orders_window o
  LEFT JOIN funnel f USING (minute_window)
  ORDER BY minute_window;
  """
  return con.execute(query).fetch_df()


def main():
  orders, events = load_sources()
  con = duckdb.connect()
  con.register("orders", orders)
  con.register("events", events)
  metrics = build_metrics(con)
  print(metrics)


if __name__ == "__main__":
  main()
```

### 示例输入
创建 `orders.csv`：

```csv
order_id,user_id,amount,channel,paid_at
1001,u01,128,app,2024-05-18T12:01:00
1002,u02,256,h5,2024-05-18T12:02:00
1003,u03,99,mini,2024-05-18T12:11:00
1004,u01,188,app,2024-05-18T12:12:00
```

创建 `events.jsonl`：

```json
{"user_id":"u01","event":"view","ts":"2024-05-18T12:00:30","channel":"app"}
{"user_id":"u01","event":"click","ts":"2024-05-18T12:00:40","channel":"app"}
{"user_id":"u02","event":"view","ts":"2024-05-18T12:01:10","channel":"h5"}
{"user_id":"u02","event":"click","ts":"2024-05-18T12:01:20","channel":"h5"}
{"user_id":"u03","event":"view","ts":"2024-05-18T12:10:50","channel":"mini"}
```

### 运行与输出

```bash
python lakehouse_lab.py
```

示例输出：

```text
       minute_window  gmv  buyers  views  clicks   arpu  conversion
0 2024-05-18 12:00:00  384       2      2       2  192.0      1.0000
1 2024-05-18 12:10:00  287       2      1       0  143.5         NaN
```

把输入文件里的时间改成任意区间，就能快速回放。示例输出中的 `conversion` 列展示出视图到支付的比率，若为 `NaN` 则意味着那段时间没有曝光，帮助运营判断是否真是“流量断崖”。

### 扩展版本：lakehouse_lab_pro.py
扩展版将指标写入本地 DuckDB 数据库，并模拟“回放任意时间段”“按渠道聚合”。它还能对比多版本口径，方便在复盘会上讨论“新版指标是否可信”。

```python
# file: lakehouse_lab_pro.py
import duckdb
import pandas as pd
from pathlib import Path

DB_PATH = Path("lakehouse.duckdb")


def prepare_db():
  con = duckdb.connect(DB_PATH)
  con.execute(
    """
    CREATE TABLE IF NOT EXISTS metrics (
      window TIMESTAMP,
      channel VARCHAR,
      gmv DOUBLE,
      buyers INTEGER,
      views INTEGER,
      clicks INTEGER,
      version INTEGER
    );
    """
  )
  return con


def ingest(con, version: int):
  orders = pd.read_csv("orders.csv", parse_dates=["paid_at"])
  events = pd.read_json("events.jsonl", lines=True)
  df = con.execute(
    """
    WITH o AS (
      SELECT DATE_TRUNC('minute', paid_at) AS window, channel, SUM(amount) AS gmv, COUNT(DISTINCT user_id) AS buyers
      FROM orders GROUP BY 1, 2
    ),
    e AS (
      SELECT DATE_TRUNC('minute', ts) AS window, channel,
             SUM(CASE WHEN event='view' THEN 1 ELSE 0 END) AS views,
             SUM(CASE WHEN event='click' THEN 1 ELSE 0 END) AS clicks
      FROM read_json_auto('events.jsonl') GROUP BY 1, 2
    )
    SELECT o.window, o.channel, o.gmv, o.buyers, COALESCE(e.views,0) AS views, COALESCE(e.clicks,0) AS clicks
    FROM o LEFT JOIN e USING (window, channel);
    """
  ).fetch_df()
  df["version"] = version
  con.execute("INSERT INTO metrics SELECT * FROM df")


def playback(con, start: str, end: str):
  query = """
  SELECT window, channel, SUM(gmv) AS gmv, SUM(buyers) AS buyers,
         SUM(views) AS views, SUM(clicks) AS clicks, version
  FROM metrics
  WHERE window BETWEEN ? AND ?
  GROUP BY 1, 2, 7
  ORDER BY window, channel;
  """
  return con.execute(query, [start, end]).fetch_df()


if __name__ == "__main__":
  con = prepare_db()
  ingest(con, version=1)
  result = playback(con, "2024-05-18 12:00:00", "2024-05-18 12:20:00")
  print(result)
```

运行：

```bash
python lakehouse_lab_pro.py
```

示例输出：

```text
              window channel    gmv  buyers  views  clicks  version
0 2024-05-18 12:00:00     app  316.0       1    1.0     1.0        1
1 2024-05-18 12:01:00      h5  256.0       1    1.0     1.0        1
2 2024-05-18 12:10:00    mini   99.0       1    1.0     0.0        1
```

你可以重复运行 `ingest(con, version=2)`，例如把 `orders.csv` 改成新的口径，再播放同一时间窗口，立刻就能看到版本差异。

## 回到复盘现场

### 湖仓联动的视角
现在再开复盘会，只需要输入时间范围，脚本就能回放指标并附带原始事件。运营不再需要在 PPT 上猜测，分析师也能专注于解释数据，而非搭桥。从 T+1 的无力感到小时级的自信，依靠的是湖仓一体化的心智模型与可复制的脚本。未来当实时特征、AI 分层指标加入这条链路时，我们已经有了一张可靠的织布地图。如果你也在推进湖仓一体指标回放，欢迎留言交流。

image_group
![湖仓跨部门协作图](https://images.unsplash.com/photo-1517430816045-df4b7de11d1d?auto=format&fit=crop&w=800&q=80)

